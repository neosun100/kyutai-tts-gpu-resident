# Kyutai TTS Docker åŒ–éƒ¨ç½²æ€»ç»“ âœ…

## ğŸ‰ å®Œæˆæƒ…å†µ

### âœ… 1. Docker åŒ–

- [x] Dockerfileï¼ˆåŸºäº NVIDIA CUDAï¼‰
- [x] docker-compose.ymlï¼ˆGPU æ”¯æŒï¼‰
- [x] .env.exampleï¼ˆç¯å¢ƒå˜é‡æ¨¡æ¿ï¼‰
- [x] start.shï¼ˆä¸€é”®å¯åŠ¨è„šæœ¬ï¼‰
- [x] stop.shï¼ˆåœæ­¢è„šæœ¬ï¼‰
- [x] .dockerignoreï¼ˆä¼˜åŒ–æ„å»ºï¼‰

**ç‰¹æ€§**ï¼š
- è‡ªåŠ¨é€‰æ‹©æ˜¾å­˜å ç”¨æœ€å°‘çš„ GPU
- è‡ªåŠ¨æ£€æµ‹ç«¯å£å†²çª
- æœåŠ¡å¯¹æ‰€æœ‰ IP å¼€æ”¾ï¼ˆ0.0.0.0ï¼‰
- æ”¯æŒå¤š GPU å¹¶è¡Œéƒ¨ç½²

### âœ… 2. GPU ç®¡ç†

- [x] gpu_manager.pyï¼ˆæ™ºèƒ½ GPU ç®¡ç†å™¨ï¼‰

**åŠŸèƒ½**ï¼š
- æ‡’åŠ è½½ï¼šé¦–æ¬¡è¯·æ±‚æ—¶åŠ è½½æ¨¡å‹
- è‡ªåŠ¨é‡Šæ”¾ï¼šç©ºé—² 60 ç§’åè‡ªåŠ¨å¸è½½
- æ‰‹åŠ¨é‡Šæ”¾ï¼šAPI ç«¯ç‚¹æ”¯æŒ
- çº¿ç¨‹å®‰å…¨ï¼šå¤šè¯·æ±‚å¹¶å‘å®‰å…¨
- å…¨å±€å…±äº«ï¼šUI/API/MCP å…±ç”¨

### âœ… 3. ä¸‰ç§è®¿é—®æ¨¡å¼

#### æ¨¡å¼ä¸€ï¼šWeb UI âœ…

- [x] ç°ä»£åŒ–å“åº”å¼ç•Œé¢
- [x] æ·±è‰²ä¸»é¢˜
- [x] ä¸­è‹±æ–‡åˆ‡æ¢
- [x] å®æ—¶ GPU çŠ¶æ€æ˜¾ç¤º
- [x] å‚æ•°å¯è°ƒï¼ˆcfg_coef, voiceï¼‰
- [x] éŸ³é¢‘åœ¨çº¿æ’­æ”¾
- [x] æ‰‹åŠ¨é‡Šæ”¾æ˜¾å­˜æŒ‰é’®

**è®¿é—®**: http://0.0.0.0:8900

#### æ¨¡å¼äºŒï¼šREST API âœ…

- [x] POST /api/ttsï¼ˆç”Ÿæˆè¯­éŸ³ï¼‰
- [x] GET /api/gpu/statusï¼ˆGPU çŠ¶æ€ï¼‰
- [x] POST /api/gpu/offloadï¼ˆé‡Šæ”¾æ˜¾å­˜ï¼‰
- [x] GET /healthï¼ˆå¥åº·æ£€æŸ¥ï¼‰
- [x] Swagger æ–‡æ¡£ï¼ˆ/apidocsï¼‰
- [x] CORS æ”¯æŒ

**ç¤ºä¾‹**ï¼š
```bash
curl -X POST http://0.0.0.0:8900/api/tts \
  -F "text=Hello" --output output.wav
```

#### æ¨¡å¼ä¸‰ï¼šMCP å·¥å…· âœ…

- [x] mcp_server.pyï¼ˆç‹¬ç«‹ MCP æœåŠ¡å™¨ï¼‰
- [x] text_to_speech å·¥å…·
- [x] get_gpu_status å·¥å…·
- [x] offload_gpu å·¥å…·
- [x] å®Œæ•´ç±»å‹æ³¨è§£
- [x] é”™è¯¯å¤„ç†
- [x] å…±äº« GPU ç®¡ç†å™¨

**é…ç½®**: mcp_config.json

### âœ… 4. æ–‡æ¡£

- [x] QUICKSTART.mdï¼ˆå¿«é€Ÿå¼€å§‹ï¼‰
- [x] README_DOCKER.mdï¼ˆå®Œæ•´éƒ¨ç½²æŒ‡å—ï¼‰
- [x] MCP_GUIDE.mdï¼ˆMCP ä½¿ç”¨æŒ‡å—ï¼‰
- [x] PROJECT_STRUCTURE.mdï¼ˆé¡¹ç›®ç»“æ„ï¼‰
- [x] DEPLOYMENT_SUMMARY.mdï¼ˆæœ¬æ–‡ä»¶ï¼‰

### âœ… 5. æµ‹è¯•å·¥å…·

- [x] test_api.shï¼ˆAPI æµ‹è¯•è„šæœ¬ï¼‰
- [x] å¥åº·æ£€æŸ¥ç«¯ç‚¹
- [x] GPU çŠ¶æ€ç›‘æ§

## ğŸ“Š æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | è¯´æ˜ |
|------|------|------|
| å®¹å™¨åŒ– | Docker + nvidia-docker | GPU æ”¯æŒ |
| Web æ¡†æ¶ | Flask | è½»é‡çº§ |
| API æ–‡æ¡£ | Flasgger (Swagger) | è‡ªåŠ¨ç”Ÿæˆ |
| MCP æ¡†æ¶ | FastMCP | æ ‡å‡†åè®® |
| æ·±åº¦å­¦ä¹  | PyTorch + CUDA | GPU åŠ é€Ÿ |
| TTS æ¨¡å‹ | Moshi (Kyutai) | 1.6B å‚æ•° |

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. ä¸€é”®å¯åŠ¨
./start.sh

# 2. è®¿é—® UI
open http://0.0.0.0:8900

# 3. æµ‹è¯• API
./test_api.sh

# 4. å¯åŠ¨ MCPï¼ˆå¯é€‰ï¼‰
python3 mcp_server.py
```

## ğŸ“ æ ¸å¿ƒæ–‡ä»¶

```
â”œâ”€â”€ app.py              # Flask åº”ç”¨ï¼ˆUI + APIï¼‰
â”œâ”€â”€ gpu_manager.py      # GPU ç®¡ç†å™¨
â”œâ”€â”€ mcp_server.py       # MCP æœåŠ¡å™¨
â”œâ”€â”€ Dockerfile          # Docker é•œåƒ
â”œâ”€â”€ docker-compose.yml  # Docker Compose
â”œâ”€â”€ start.sh            # ä¸€é”®å¯åŠ¨
â””â”€â”€ requirements.txt    # Python ä¾èµ–
```

## ğŸ¯ å…³é”®ç‰¹æ€§

### 1. æ™ºèƒ½ GPU ç®¡ç†

```python
# è‡ªåŠ¨é€‰æ‹©æœ€ç©ºé—²çš„ GPU
GPU_ID=$(nvidia-smi --query-gpu=memory.used \
         --format=csv,noheader,nounits | \
         sort -n | head -1 | cut -d',' -f1)
```

### 2. æ‡’åŠ è½½ + è‡ªåŠ¨é‡Šæ”¾

```python
class GPUManager:
    def get_model(self, load_func):
        if self.model is None:
            self.model = load_func()  # æ‡’åŠ è½½
        return self.model
    
    def _monitor(self):
        if idle_time > timeout:
            self.force_offload()  # è‡ªåŠ¨é‡Šæ”¾
```

### 3. å•ç«¯å£å¤šåŠŸèƒ½

```
http://0.0.0.0:8900/
â”œâ”€â”€ /              â†’ UI ç•Œé¢
â”œâ”€â”€ /api/tts       â†’ REST API
â”œâ”€â”€ /apidocs       â†’ Swagger æ–‡æ¡£
â””â”€â”€ /health        â†’ å¥åº·æ£€æŸ¥
```

### 4. ä¸‰æ¨¡å¼å…±äº«èµ„æº

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU Manager (Singleton)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“           â†“           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚  UI  â”‚   â”‚ API  â”‚   â”‚ MCP  â”‚
â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ é…ç½®ç¤ºä¾‹

### ç¯å¢ƒå˜é‡ (.env)

```bash
PORT=8900
GPU_IDLE_TIMEOUT=60
NVIDIA_VISIBLE_DEVICES=2
HF_REPO=kyutai/tts-1.6b
```

### å¤š GPU éƒ¨ç½²

```bash
# GPU 0
NVIDIA_VISIBLE_DEVICES=0 PORT=8900 docker-compose up -d

# GPU 1
NVIDIA_VISIBLE_DEVICES=1 PORT=8901 docker-compose up -d
```

### MCP é…ç½®

```json
{
  "mcpServers": {
    "kyutai-tts": {
      "command": "python3",
      "args": ["mcp_server.py"],
      "env": {
        "GPU_IDLE_TIMEOUT": "600"
      }
    }
  }
}
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ¨¡å‹å¤§å° | 1.6B å‚æ•° |
| æ˜¾å­˜å ç”¨ | 3-4GB |
| å»¶è¿Ÿ | 350ms (L40S, 32å¹¶å‘) |
| ç”Ÿæˆé€Ÿåº¦ | å®æ—¶ 3-5x |
| æ”¯æŒè¯­è¨€ | è‹±è¯­ã€æ³•è¯­ |

## ğŸ§ª æµ‹è¯•æ¸…å•

- [x] Docker é•œåƒæ„å»ºæˆåŠŸ
- [x] å®¹å™¨å¯åŠ¨æˆåŠŸ
- [x] è‡ªåŠ¨é€‰æ‹©æœ€ç©ºé—² GPU
- [x] UI ç•Œé¢å¯è®¿é—®
- [x] API æ¥å£å¯è®¿é—®
- [x] Swagger æ–‡æ¡£å¯è®¿é—®
- [x] MCP æœåŠ¡å™¨å¯è¿æ¥
- [x] MCP å·¥å…·å¯è°ƒç”¨
- [x] å¤šè¯­è¨€åˆ‡æ¢æ­£å¸¸
- [x] GPU è‡ªåŠ¨é‡Šæ”¾æ­£å¸¸
- [x] æ‰‹åŠ¨é‡Šæ”¾æ˜¾å­˜æ­£å¸¸

## ğŸ“ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šå¿«é€Ÿä½“éªŒ

```bash
./start.sh
# æ‰“å¼€æµè§ˆå™¨è®¿é—® UI
```

### åœºæ™¯ 2ï¼šAPI é›†æˆ

```python
import requests

response = requests.post(
    'http://0.0.0.0:8900/api/tts',
    data={'text': 'Hello, world!'}
)
```

### åœºæ™¯ 3ï¼šAI Agent

```python
# é€šè¿‡ MCP è°ƒç”¨
result = await mcp_client.call_tool(
    "text_to_speech",
    {"text": "Hello", "output_path": "/tmp/out.wav"}
)
```

### åœºæ™¯ 4ï¼šç”Ÿäº§éƒ¨ç½²

```bash
# å¤š GPU + Nginx è´Ÿè½½å‡è¡¡
for gpu in 0 1 2 3; do
    NVIDIA_VISIBLE_DEVICES=$gpu \
    PORT=$((8900+gpu)) \
    docker-compose up -d
done
```

## ğŸ” ç›‘æ§å’Œç»´æŠ¤

### æŸ¥çœ‹æ—¥å¿—

```bash
docker-compose logs -f
```

### GPU ç›‘æ§

```bash
watch -n 1 nvidia-smi
```

### å®¹å™¨çŠ¶æ€

```bash
docker-compose ps
docker stats kyutai-tts
```

### é‡Šæ”¾èµ„æº

```bash
# API æ–¹å¼
curl -X POST http://0.0.0.0:8900/api/gpu/offload

# é‡å¯å®¹å™¨
docker-compose restart
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ç«¯å£è¢«å ç”¨

```bash
# è‡ªåŠ¨é€‰æ‹©å¯ç”¨ç«¯å£
./start.sh  # ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å¯ç”¨ç«¯å£
```

### Q2: æ˜¾å­˜ä¸è¶³

```bash
# é‡Šæ”¾æ˜¾å­˜
curl -X POST http://0.0.0.0:8900/api/gpu/offload
```

### Q3: æ¨¡å‹ä¸‹è½½æ…¢

```bash
# ä½¿ç”¨é•œåƒç«™
export HF_ENDPOINT=https://hf-mirror.com
./start.sh
```

### Q4: å®¹å™¨æ— æ³•å¯åŠ¨

```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker-compose logs

# æ£€æŸ¥ nvidia-docker
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

## ğŸ“š æ–‡æ¡£å¯¼èˆª

| æ–‡æ¡£ | ç”¨é€” |
|------|------|
| [QUICKSTART.md](QUICKSTART.md) | å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èï¼‰ |
| [README_DOCKER.md](README_DOCKER.md) | å®Œæ•´éƒ¨ç½²æŒ‡å— |
| [MCP_GUIDE.md](MCP_GUIDE.md) | MCP å·¥å…·ä½¿ç”¨ |
| [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) | é¡¹ç›®ç»“æ„è¯´æ˜ |
| [README.md](README.md) | åŸé¡¹ç›®è¯´æ˜ |

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **åŸºç¡€ä½¿ç”¨**: é˜…è¯» [QUICKSTART.md](QUICKSTART.md)
2. **æ·±å…¥é…ç½®**: é˜…è¯» [README_DOCKER.md](README_DOCKER.md)
3. **MCP é›†æˆ**: é˜…è¯» [MCP_GUIDE.md](MCP_GUIDE.md)
4. **ç”Ÿäº§éƒ¨ç½²**: é…ç½®è´Ÿè½½å‡è¡¡å’Œç›‘æ§

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

- Python ä»£ç : MIT License
- Rust ä»£ç : Apache License
- æ¨¡å‹æƒé‡: CC-BY 4.0

---

**éƒ¨ç½²å®Œæˆï¼äº«å— Kyutai TTS å¸¦æ¥çš„é«˜è´¨é‡è¯­éŸ³åˆæˆä½“éªŒï¼ğŸ‰**
